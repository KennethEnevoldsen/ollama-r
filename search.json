[{"path":"https://hauselin.github.io/ollama-r/LICENSE.html","id":null,"dir":"","previous_headings":"","what":"MIT License","title":"MIT License","text":"Copyright (c) 2024 ollamar authors Permission hereby granted, free charge, person obtaining copy software associated documentation files (“Software”), deal Software without restriction, including without limitation rights use, copy, modify, merge, publish, distribute, sublicense, /sell copies Software, permit persons Software furnished , subject following conditions: copyright notice permission notice shall included copies substantial portions Software. SOFTWARE PROVIDED “”, WITHOUT WARRANTY KIND, EXPRESS IMPLIED, INCLUDING LIMITED WARRANTIES MERCHANTABILITY, FITNESS PARTICULAR PURPOSE NONINFRINGEMENT. EVENT SHALL AUTHORS COPYRIGHT HOLDERS LIABLE CLAIM, DAMAGES LIABILITY, WHETHER ACTION CONTRACT, TORT OTHERWISE, ARISING , CONNECTION SOFTWARE USE DEALINGS SOFTWARE.","code":""},{"path":"https://hauselin.github.io/ollama-r/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Hause Lin. Author, maintainer, copyright holder.","code":""},{"path":"https://hauselin.github.io/ollama-r/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Lin H (2024). ollamar: 'Ollama' Language Models. R package version 1.1.1, https://github.com/hauselin/ollama-r, https://hauselin.github.io/ollama-r/.","code":"@Manual{,   title = {ollamar: 'Ollama' Language Models},   author = {Hause Lin},   year = {2024},   note = {R package version 1.1.1, https://github.com/hauselin/ollama-r},   url = {https://hauselin.github.io/ollama-r/}, }"},{"path":"https://hauselin.github.io/ollama-r/index.html","id":"ollama-r-library","dir":"","previous_headings":"","what":"Ollama Language Models","title":"Ollama Language Models","text":"Ollama R library provides easiest way integrate R Ollama, lets run language models locally machine. Main site: https://hauselin.github.io/ollama-r/ Note: least 8 GB RAM available run 7B models, 16 GB run 13B models, 32 GB run 33B models. See Ollama’s Github page information. See also Ollama API documentation endpoints. Ollama Python, see ollama-python. ’ll need Ollama app installed computer use library.","code":""},{"path":"https://hauselin.github.io/ollama-r/index.html","id":"installation","dir":"","previous_headings":"","what":"Installation","title":"Ollama Language Models","text":"Ollama app installed computer. Download Ollama. Open/launch Ollama app start local server. can run language models locally, machine/computer. Install stable version like : Alternatively, latest/development version /latest features, can install like : doesn’t work don’t devtools installed, please run install.packages(\"devtools\") R RStudio first.","code":"install.packages(\"ollamar\") devtools::install_github(\"hauselin/ollamar\")"},{"path":"https://hauselin.github.io/ollama-r/index.html","id":"usage","dir":"","previous_headings":"","what":"Usage","title":"Ollama Language Models","text":"ollamar uses httr2 library make HTTP requests Ollama server, many functions library returns httr2_response object default. See Notes section information.","code":"library(ollamar)  test_connection()  # test connection to Ollama server; returns a httr2 response object # Ollama local server running successfully # <httr2_response>  list_models()  # list available models (models you've pulled/downloaded)    name                     size    parameter_size quantization_level modified    <chr>                    <chr>   <chr>          <chr>              <chr>  1 llama3:latest            4.7 GB  8B             Q4_0               2024-05-01T21:01:00  2 mistral-openorca:latest  4.1 GB  7B             Q4_0               2024-04-25T16:45:00"},{"path":"https://hauselin.github.io/ollama-r/index.html","id":"pulldownload-model","dir":"","previous_headings":"Usage","what":"Pull/download model","title":"Ollama Language Models","text":"Download model ollama library (see API doc). list models can pull/download, see Ollama library.","code":"pull(\"llama3\")  # pull/download llama3 model pull(\"mistral-openorca\")  # pull/download mistral-openorca model list_models()  # verify you've pulled/downloaded the model"},{"path":"https://hauselin.github.io/ollama-r/index.html","id":"delete-a-model","dir":"","previous_headings":"Usage","what":"Delete a model","title":"Ollama Language Models","text":"Delete model data (see API doc). can see models ’ve downloaded list_models(). download model, specify name model.","code":"list_models()  # see the models you've pulled/downloaded delete(\"all-minilm:latest\")  # returns a httr2 response object"},{"path":"https://hauselin.github.io/ollama-r/index.html","id":"generate-a-completion","dir":"","previous_headings":"Usage","what":"Generate a completion","title":"Ollama Language Models","text":"Generate response given prompt (see API doc).","code":"resp <- generate(\"llama3\", \"Tomorrow is a...\")  # return httr2 response object by default resp  resp_process(resp, \"text\")  # process the response to return text/vector output  generate(\"llama3\", \"Tomorrow is a...\", output = \"text\")  # directly return text/vector output generate(\"llama3\", \"Tomorrow is a...\", stream = TRUE)  # return httr2 response object and stream output generate(\"llama3\", \"Tomorrow is a...\", output = \"df\", stream = TRUE)"},{"path":"https://hauselin.github.io/ollama-r/index.html","id":"chat","dir":"","previous_headings":"Usage","what":"Chat","title":"Ollama Language Models","text":"Generate next message chat (see API doc). See Notes section utility/helper functions help format/prepare messages functions/API calls.","code":"messages <- list(     list(role = \"user\", content = \"Who is the prime minister of the uk?\") ) resp <- chat(\"llama3\", messages)  # default returns httr2 response object resp  # <httr2_response> resp_process(resp, \"text\")  # process the response to return text/vector output  # specify output type when calling the function chat(\"llama3\", messages, output = \"text\")  # text vector chat(\"llama3\", messages, output = \"df\")  # data frame/tibble chat(\"llama3\", messages, output = \"jsonlist\")  # list chat(\"llama3\", messages, output = \"raw\")  # raw string chat(\"llama3\", messages, stream = TRUE)  # stream output and return httr2 response object  # list of messages messages <- list(     list(role = \"user\", content = \"Hello!\"),     list(role = \"assistant\", content = \"Hi! How are you?\"),     list(role = \"user\", content = \"Who is the prime minister of the uk?\"),     list(role = \"assistant\", content = \"Rishi Sunak\"),     list(role = \"user\", content = \"List all the previous messages.\") ) cat(chat(\"llama3\", messages, output = \"text\"))  # print the formatted output"},{"path":"https://hauselin.github.io/ollama-r/index.html","id":"streaming-responses","dir":"","previous_headings":"Usage > Chat","what":"Streaming responses","title":"Ollama Language Models","text":"","code":"messages <- list(     list(role = \"user\", content = \"Hello!\"),     list(role = \"assistant\", content = \"Hi! How are you?\"),     list(role = \"user\", content = \"Who is the prime minister of the uk?\"),     list(role = \"assistant\", content = \"Rishi Sunak\"),     list(role = \"user\", content = \"List all the previous messages.\") )  # use \"llama3\" model, provide list of messages, return text/vector output, and stream the output chat(\"llama3\", messages, output = \"text\", stream = TRUE) # chat(model = \"llama3\", messages = messages, output = \"text\", stream = TRUE)  # same as above"},{"path":"https://hauselin.github.io/ollama-r/index.html","id":"embeddings","dir":"","previous_headings":"Usage","what":"Embeddings","title":"Ollama Language Models","text":"Get vector embedding prompt/text (see API doc). default, embeddings normalized length 1, means following: cosine similarity can computed slightly faster using just dot product cosine similarity Euclidean distance result identical rankings","code":"embed(\"llama3\", \"Hello, how are you?\")  # don't normalize embeddings embed(\"llama3\", \"Hello, how are you?\", normalize = FALSE) # get embeddings for similar prompts e1 <- embed(\"llama3\", \"Hello, how are you?\") e2 <- embed(\"llama3\", \"Hi, how are you?\")  # compute cosine similarity sum(e1 * e2)  # not equals to 1 sum(e1 * e1)  # 1 (identical vectors/embeddings)  # non-normalized embeddings e3 <- embed(\"llama3\", \"Hello, how are you?\", normalize = FALSE) e4 <- embed(\"llama3\", \"Hi, how are you?\", normalize = FALSE)"},{"path":"https://hauselin.github.io/ollama-r/index.html","id":"notes","dir":"","previous_headings":"Usage","what":"Notes","title":"Ollama Language Models","text":"don’t Ollama app running, ’ll get error. Make sure open Ollama app using library.","code":"test_connection() # Ollama local server not running or wrong server. # Error in `httr2::req_perform()` at ollamar/R/test_connection.R:18:9:"},{"path":"https://hauselin.github.io/ollama-r/index.html","id":"parsing-httr2_response-objects-with-resp_process","dir":"","previous_headings":"Usage > Notes","what":"Parsing httr2_response objects with resp_process()","title":"Ollama Language Models","text":"ollamar uses httr2 library make HTTP requests Ollama server, many functions library returns httr2_response object default. can either parse output resp_process() use output parameter function specify output format. Generally, output parameter can one \"df\", \"jsonlist\", \"raw\", \"resp\", \"text\".","code":"resp <- list_models(output = \"resp\")  # returns a httr2 response object # <httr2_response> # GET http://127.0.0.1:11434/api/tags # Status: 200 OK # Content-Type: application/json # Body: In memory (5401 bytes)  # process the httr2 response object with the resp_process() function resp_process(resp, \"df\") # or list_models(output = \"df\") resp_process(resp, \"jsonlist\")  # list # or list_models(output = \"jsonlist\") resp_process(resp, \"raw\")  # raw string # or list_models(output = \"raw\") resp_process(resp, \"resp\")  # returns the input httr2 response object # or list_models() or list_models(\"resp\") resp_process(resp, \"text\")  # text vector # or list_models(\"text\")"},{"path":"https://hauselin.github.io/ollama-r/index.html","id":"utilityhelper-functions-to-format-and-prepare-messages-for-the-chat-function","dir":"","previous_headings":"Usage > Notes","what":"Utility/helper functions to format and prepare messages for the chat() function","title":"Ollama Language Models","text":"Internally, messages represented list many distinct list messages. list/message object two elements: role (can \"user\" \"assistant\" \"system\") content (message text). example shows messages/lists presented. simplify process creating managing messages, ollamar provides utility/helper functions format prepare messages chat() function. create_message() creates first message append_message() adds new message end existing messages prepend_message() adds new message beginning existing messages default, inserts message -1 (final) position positive negative indices/positions supported 5 messages, positions 1 (-5), 2 (-4), 3 (-3), 4 (-2), 5 (-1)","code":"list(  # main list containing all the messages     list(role = \"user\", content = \"Hello!\"),  # first message as a list     list(role = \"assistant\", content = \"Hi! How are you?\"),  # second message as a list     list(role = \"user\", content = \"Who is the prime minister of the uk?\"),  # third message as a list     list(role = \"assistant\", content = \"Rishi Sunak\"),  # fourth message as a list     list(role = \"user\", content = \"List all the previous messages.\")  # fifth message as a list ) # create first message messages <- create_message(content = \"Hi! How are you? (1ST MESSAGE)\", role = \"assistant\") # or simply, messages <- create_message(\"Hi! How are you?\", \"assistant\") messages[[1]]  # get 1st message  # append (add to the end) a new message to the existing messages messages <- append_message(\"I'm good. How are you? (2ND MESSAGE)\", \"user\", messages) messages[[1]]  # get 1st message messages[[2]]  # get 2nd message (newly added message)  # prepend (add to the beginning) a new message to the existing messages messages <- prepend_message(\"I'm good. How are you? (0TH MESSAGE)\", \"user\", messages) messages[[1]]  # get 0th message (newly added message) messages[[2]]  # get 1st message messages[[3]]  # get 2nd message  # insert a new message at a specific index/position (2nd position in the example below) # by default, the message is inserted at the end of the existing messages (position -1 is the end/default) messages <- insert_message(\"I'm good. How are you? (BETWEEN 0 and 1 MESSAGE)\", \"user\", messages, 2) messages[[1]]  # get 0th message messages[[2]]  # get between 0 and 1 message (newly added message) messages[[3]]  # get 1st message messages[[4]]  # get 2nd message  # delete a message at a specific index/position (2nd position in the example below) messages <- delete_message(messages, 2)"},{"path":"https://hauselin.github.io/ollama-r/reference/append_message.html","id":null,"dir":"Reference","previous_headings":"","what":"Append message to a list — append_message","title":"Append message to a list — append_message","text":"Appends message (add end list) list messages. role content converted list appended input list.","code":""},{"path":"https://hauselin.github.io/ollama-r/reference/append_message.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Append message to a list — append_message","text":"","code":"append_message(content, role = \"user\", x = NULL)"},{"path":"https://hauselin.github.io/ollama-r/reference/append_message.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Append message to a list — append_message","text":"content content message. role role message. Can \"user\", \"system\", \"assistant\". Default \"user\". x list messages. Default NULL.","code":""},{"path":"https://hauselin.github.io/ollama-r/reference/append_message.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Append message to a list — append_message","text":"list messages new message appended.","code":""},{"path":"https://hauselin.github.io/ollama-r/reference/append_message.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Append message to a list — append_message","text":"","code":"append_message(\"user\", \"Hello\") #> [[1]] #> [[1]]$role #> [1] \"Hello\" #>  #> [[1]]$content #> [1] \"user\" #>  #>  append_message(\"system\", \"Always respond nicely\") #> [[1]] #> [[1]]$role #> [1] \"Always respond nicely\" #>  #> [[1]]$content #> [1] \"system\" #>  #>"},{"path":"https://hauselin.github.io/ollama-r/reference/chat.html","id":null,"dir":"Reference","previous_headings":"","what":"Chat with Ollama models — chat","title":"Chat with Ollama models — chat","text":"Chat Ollama models","code":""},{"path":"https://hauselin.github.io/ollama-r/reference/chat.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Chat with Ollama models — chat","text":"","code":"chat(   model,   messages,   tools = list(),   stream = FALSE,   keep_alive = \"5m\",   output = c(\"resp\", \"jsonlist\", \"raw\", \"df\", \"text\"),   endpoint = \"/api/chat\",   host = NULL,   ... )"},{"path":"https://hauselin.github.io/ollama-r/reference/chat.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Chat with Ollama models — chat","text":"model character string model name \"llama3\". messages list list messages model (see examples ). tools Tools model use supported. Requires stream = FALSE. Default empty list. stream Enable response streaming. Default FALSE. keep_alive duration keep connection alive. Default \"5m\". output output format. Default \"resp\". options \"jsonlist\", \"raw\", \"df\", \"text\". endpoint endpoint chat model. Default \"/api/chat\". host base URL use. Default NULL, uses Ollama's default base URL. ... Additional options pass model.","code":""},{"path":"https://hauselin.github.io/ollama-r/reference/chat.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Chat with Ollama models — chat","text":"response format specified output parameter.","code":""},{"path":"https://hauselin.github.io/ollama-r/reference/chat.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Chat with Ollama models — chat","text":"API documentation","code":""},{"path":"https://hauselin.github.io/ollama-r/reference/chat.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Chat with Ollama models — chat","text":"","code":"if (FALSE) { # test_connection()$status_code == 200 # one message messages <- list(     list(role = \"user\", content = \"How are you doing?\") ) chat(\"llama3\", messages) # returns response by default chat(\"llama3\", messages, output = \"text\") # returns text/vector chat(\"llama3\", messages, temperature = 2.8) # additional options chat(\"llama3\", messages, stream = TRUE) # stream response chat(\"llama3\", messages, output = \"df\", stream = TRUE) # stream and return dataframe  # multiple messages messages <- list(     list(role = \"user\", content = \"Hello!\"),     list(role = \"assistant\", content = \"Hi! How are you?\"),     list(role = \"user\", content = \"Who is the prime minister of the uk?\"),     list(role = \"assistant\", content = \"Rishi Sunak\"),     list(role = \"user\", content = \"List all the previous messages.\") ) chat(\"llama3\", messages, stream = TRUE) }"},{"path":"https://hauselin.github.io/ollama-r/reference/check_option_valid.html","id":null,"dir":"Reference","previous_headings":"","what":"Check if an option is valid. — check_option_valid","title":"Check if an option is valid. — check_option_valid","text":"Check option valid.","code":""},{"path":"https://hauselin.github.io/ollama-r/reference/check_option_valid.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Check if an option is valid. — check_option_valid","text":"","code":"check_option_valid(opt)"},{"path":"https://hauselin.github.io/ollama-r/reference/check_option_valid.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Check if an option is valid. — check_option_valid","text":"opt option (character) check.","code":""},{"path":"https://hauselin.github.io/ollama-r/reference/check_option_valid.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Check if an option is valid. — check_option_valid","text":"Returns TRUE option valid, FALSE otherwise.","code":""},{"path":"https://hauselin.github.io/ollama-r/reference/check_option_valid.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Check if an option is valid. — check_option_valid","text":"","code":"check_option_valid(\"mirostat\") #> [1] TRUE check_option_valid(\"invalid_option\") #> [1] FALSE"},{"path":"https://hauselin.github.io/ollama-r/reference/check_options.html","id":null,"dir":"Reference","previous_headings":"","what":"Check if a vector of options are valid. — check_options","title":"Check if a vector of options are valid. — check_options","text":"Check vector options valid.","code":""},{"path":"https://hauselin.github.io/ollama-r/reference/check_options.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Check if a vector of options are valid. — check_options","text":"","code":"check_options(opts = NULL)"},{"path":"https://hauselin.github.io/ollama-r/reference/check_options.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Check if a vector of options are valid. — check_options","text":"opts vector options check.","code":""},{"path":"https://hauselin.github.io/ollama-r/reference/check_options.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Check if a vector of options are valid. — check_options","text":"Returns list two elements: valid_options invalid_options.","code":""},{"path":"https://hauselin.github.io/ollama-r/reference/check_options.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Check if a vector of options are valid. — check_options","text":"","code":"check_options(c(\"mirostat\", \"invalid_option\")) #> $valid_options #> [1] \"mirostat\" #>  #> $invalid_options #> [1] \"invalid_option\" #>  check_options(c(\"mirostat\", \"num_predict\")) #> $valid_options #> [1] \"mirostat\"    \"num_predict\" #>  #> $invalid_options #> character(0) #>"},{"path":"https://hauselin.github.io/ollama-r/reference/create_message.html","id":null,"dir":"Reference","previous_headings":"","what":"Create a message — create_message","title":"Create a message — create_message","text":"Create message","code":""},{"path":"https://hauselin.github.io/ollama-r/reference/create_message.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create a message — create_message","text":"","code":"create_message(content, role = \"user\")"},{"path":"https://hauselin.github.io/ollama-r/reference/create_message.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create a message — create_message","text":"content content message. role role message. Can \"user\", \"system\", \"assistant\". Default \"user\".","code":""},{"path":"https://hauselin.github.io/ollama-r/reference/create_message.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create a message — create_message","text":"list messages.","code":""},{"path":"https://hauselin.github.io/ollama-r/reference/create_message.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create a message — create_message","text":"","code":"create_message(\"Hello\", \"user\") #> [[1]] #> [[1]]$role #> [1] \"user\" #>  #> [[1]]$content #> [1] \"Hello\" #>  #>  create_message(\"Always respond nicely\", \"system\") #> [[1]] #> [[1]]$role #> [1] \"system\" #>  #> [[1]]$content #> [1] \"Always respond nicely\" #>  #>  create_message(\"I am here to help\", \"assistant\") #> [[1]] #> [[1]]$role #> [1] \"assistant\" #>  #> [[1]]$content #> [1] \"I am here to help\" #>  #>"},{"path":"https://hauselin.github.io/ollama-r/reference/create_request.html","id":null,"dir":"Reference","previous_headings":"","what":"Create a httr2 request object. — create_request","title":"Create a httr2 request object. — create_request","text":"Creates httr2 request object base URL, headers endpoint. Used functions package intended used directly.","code":""},{"path":"https://hauselin.github.io/ollama-r/reference/create_request.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create a httr2 request object. — create_request","text":"","code":"create_request(endpoint, host = NULL)"},{"path":"https://hauselin.github.io/ollama-r/reference/create_request.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create a httr2 request object. — create_request","text":"endpoint endpoint create request host base URL use. Default NULL, uses http://127.0.0.1:11434","code":""},{"path":"https://hauselin.github.io/ollama-r/reference/create_request.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create a httr2 request object. — create_request","text":"httr2 request object.","code":""},{"path":"https://hauselin.github.io/ollama-r/reference/create_request.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create a httr2 request object. — create_request","text":"","code":"create_request(\"/api/tags\") #> <httr2_request> #> GET http://127.0.0.1:11434/api/tags #> Headers: #> • content_type: 'application/json' #> • accept: 'application/json' #> • user_agent: 'ollama-r/1.1.1 (x86_64-pc-linux-gnu) R/4.4.1' #> Body: empty create_request(\"/api/chat\") #> <httr2_request> #> GET http://127.0.0.1:11434/api/chat #> Headers: #> • content_type: 'application/json' #> • accept: 'application/json' #> • user_agent: 'ollama-r/1.1.1 (x86_64-pc-linux-gnu) R/4.4.1' #> Body: empty create_request(\"/api/embeddings\") #> <httr2_request> #> GET http://127.0.0.1:11434/api/embeddings #> Headers: #> • content_type: 'application/json' #> • accept: 'application/json' #> • user_agent: 'ollama-r/1.1.1 (x86_64-pc-linux-gnu) R/4.4.1' #> Body: empty"},{"path":"https://hauselin.github.io/ollama-r/reference/delete.html","id":null,"dir":"Reference","previous_headings":"","what":"Delete a model — delete","title":"Delete a model — delete","text":"Delete model local machine downlaoded using pull() function. see models available, use list_models() function.","code":""},{"path":"https://hauselin.github.io/ollama-r/reference/delete.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Delete a model — delete","text":"","code":"delete(name, endpoint = \"/api/delete\", host = NULL)"},{"path":"https://hauselin.github.io/ollama-r/reference/delete.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Delete a model — delete","text":"name character string model name \"llama3\". endpoint endpoint delete model. Default \"/api/delete\". host base URL use. Default NULL, uses Ollama's default base URL.","code":""},{"path":"https://hauselin.github.io/ollama-r/reference/delete.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Delete a model — delete","text":"httr2 response object.","code":""},{"path":"https://hauselin.github.io/ollama-r/reference/delete.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Delete a model — delete","text":"API documentation","code":""},{"path":"https://hauselin.github.io/ollama-r/reference/delete.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Delete a model — delete","text":"","code":"if (FALSE) { # \\dontrun{ delete(\"llama3\") } # }"},{"path":"https://hauselin.github.io/ollama-r/reference/delete_message.html","id":null,"dir":"Reference","previous_headings":"","what":"Delete a message in a specified position from a list — delete_message","title":"Delete a message in a specified position from a list — delete_message","text":"Delete message using positive negative positions/indices. Negative positions/indices can used refer elements/messages end sequence.","code":""},{"path":"https://hauselin.github.io/ollama-r/reference/delete_message.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Delete a message in a specified position from a list — delete_message","text":"","code":"delete_message(x, position = -1)"},{"path":"https://hauselin.github.io/ollama-r/reference/delete_message.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Delete a message in a specified position from a list — delete_message","text":"x list messages. position position message delete.","code":""},{"path":"https://hauselin.github.io/ollama-r/reference/delete_message.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Delete a message in a specified position from a list — delete_message","text":"list messages message specified position removed.","code":""},{"path":"https://hauselin.github.io/ollama-r/reference/delete_message.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Delete a message in a specified position from a list — delete_message","text":"","code":"messages <- list(     list(role = \"system\", content = \"Be friendly\"),     list(role = \"user\", content = \"How are you?\") ) delete_message(messages, 1) # delete first message #> [[1]] #> [[1]]$role #> [1] \"user\" #>  #> [[1]]$content #> [1] \"How are you?\" #>  #>  delete_message(messages, -2) # same as above (delete first message) #> [[1]] #> [[1]]$role #> [1] \"user\" #>  #> [[1]]$content #> [1] \"How are you?\" #>  #>  delete_message(messages, 2) # delete second message #> [[1]] #> [[1]]$role #> [1] \"system\" #>  #> [[1]]$content #> [1] \"Be friendly\" #>  #>  delete_message(messages, -1) # same as above (delete second message) #> [[1]] #> [[1]]$role #> [1] \"system\" #>  #> [[1]]$content #> [1] \"Be friendly\" #>  #>"},{"path":"https://hauselin.github.io/ollama-r/reference/embed.html","id":null,"dir":"Reference","previous_headings":"","what":"Get embedding for inputs — embed","title":"Get embedding for inputs — embed","text":"Supercedes embeddings() function.","code":""},{"path":"https://hauselin.github.io/ollama-r/reference/embed.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get embedding for inputs — embed","text":"","code":"embed(   model,   input,   truncate = TRUE,   normalize = TRUE,   keep_alive = \"5m\",   endpoint = \"/api/embed\",   host = NULL,   ... )"},{"path":"https://hauselin.github.io/ollama-r/reference/embed.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get embedding for inputs — embed","text":"model character string model name \"llama3\". input vector characters want get embeddings . truncate Truncates end input fit within context length. Returns error FALSE context length exceeded. Defaults TRUE. normalize Normalize vector length 1. Default TRUE. keep_alive time keep connection alive. Default \"5m\" (5 minutes). endpoint endpoint get vector embedding. Default \"/api/embeddings\". host base URL use. Default NULL, uses Ollama's default base URL. ... Additional options pass model.","code":""},{"path":"https://hauselin.github.io/ollama-r/reference/embed.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get embedding for inputs — embed","text":"numeric matrix embedding. column embedding one input.","code":""},{"path":"https://hauselin.github.io/ollama-r/reference/embed.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Get embedding for inputs — embed","text":"API documentation","code":""},{"path":"https://hauselin.github.io/ollama-r/reference/embed.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Get embedding for inputs — embed","text":"","code":"if (FALSE) { # test_connection()$status_code == 200 embed(\"nomic-embed-text:latest\", \"The quick brown fox jumps over the lazy dog.\") # pass multiple inputs embed(\"nomic-embed-text:latest\", c(\"Good bye\", \"Bye\", \"See you.\")) # pass model options to the model embed(\"nomic-embed-text:latest\", \"Hello!\", temperature = 0.1, num_predict = 3) }"},{"path":"https://hauselin.github.io/ollama-r/reference/embeddings.html","id":null,"dir":"Reference","previous_headings":"","what":"Get vector embedding for a single prompt - deprecated in favor of embed() — embeddings","title":"Get vector embedding for a single prompt - deprecated in favor of embed() — embeddings","text":"function deprecated time superceded embed(). See embed() details.","code":""},{"path":"https://hauselin.github.io/ollama-r/reference/embeddings.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get vector embedding for a single prompt - deprecated in favor of embed() — embeddings","text":"","code":"embeddings(   model,   prompt,   normalize = TRUE,   keep_alive = \"5m\",   endpoint = \"/api/embeddings\",   host = NULL,   ... )"},{"path":"https://hauselin.github.io/ollama-r/reference/embeddings.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get vector embedding for a single prompt - deprecated in favor of embed() — embeddings","text":"model character string model name \"llama3\". prompt character string prompt want get vector embedding . normalize Normalize vector length 1. Default TRUE. keep_alive time keep connection alive. Default \"5m\" (5 minutes). endpoint endpoint get vector embedding. Default \"/api/embeddings\". host base URL use. Default NULL, uses Ollama's default base URL. ... Additional options pass model.","code":""},{"path":"https://hauselin.github.io/ollama-r/reference/embeddings.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get vector embedding for a single prompt - deprecated in favor of embed() — embeddings","text":"numeric vector embedding.","code":""},{"path":"https://hauselin.github.io/ollama-r/reference/embeddings.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Get vector embedding for a single prompt - deprecated in favor of embed() — embeddings","text":"API documentation","code":""},{"path":"https://hauselin.github.io/ollama-r/reference/embeddings.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Get vector embedding for a single prompt - deprecated in favor of embed() — embeddings","text":"","code":"if (FALSE) { # test_connection()$status_code == 200 embeddings(\"nomic-embed-text:latest\", \"The quick brown fox jumps over the lazy dog.\") # pass model options to the model embeddings(\"nomic-embed-text:latest\", \"Hello!\", temperature = 0.1, num_predict = 3) }"},{"path":"https://hauselin.github.io/ollama-r/reference/generate.html","id":null,"dir":"Reference","previous_headings":"","what":"Generate a completion. — generate","title":"Generate a completion. — generate","text":"Generate response given prompt provided model.","code":""},{"path":"https://hauselin.github.io/ollama-r/reference/generate.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Generate a completion. — generate","text":"","code":"generate(   model,   prompt,   suffix = \"\",   images = \"\",   system = \"\",   template = \"\",   context = list(),   stream = FALSE,   raw = FALSE,   keep_alive = \"5m\",   output = c(\"resp\", \"jsonlist\", \"raw\", \"df\", \"text\"),   endpoint = \"/api/generate\",   host = NULL,   ... )"},{"path":"https://hauselin.github.io/ollama-r/reference/generate.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Generate a completion. — generate","text":"model character string model name \"llama3\". prompt character string promp like \"sky ...\" suffix character string model response. Default \"\". images path image file include prompt. Default \"\". system character string system prompt (overrides defined Modelfile). Default \"\". template character string prompt template (overrides defined Modelfile). Default \"\". context list context previous response include previous conversation prompt. Default empty list. stream Enable response streaming. Default FALSE. raw TRUE, formatting applied prompt. may choose use raw parameter specifying full templated prompt request API. Default FALSE. keep_alive time keep connection alive. Default \"5m\" (5 minutes). output character vector output format. Default \"resp\". Options \"resp\", \"jsonlist\", \"raw\", \"df\", \"text\". endpoint endpoint generate completion. Default \"/api/generate\". host base URL use. Default NULL, uses Ollama's default base URL. ... Additional options pass model.","code":""},{"path":"https://hauselin.github.io/ollama-r/reference/generate.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Generate a completion. — generate","text":"response format specified output parameter.","code":""},{"path":"https://hauselin.github.io/ollama-r/reference/generate.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Generate a completion. — generate","text":"API documentation","code":""},{"path":"https://hauselin.github.io/ollama-r/reference/generate.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Generate a completion. — generate","text":"","code":"if (FALSE) { # test_connection()$status_code == 200 generate(\"llama3\", \"The sky is...\", stream = FALSE, output = \"df\") generate(\"llama3\", \"The sky is...\", stream = TRUE, output = \"text\") generate(\"llama3\", \"The sky is...\", stream = TRUE, output = \"text\", temperature = 2.0) generate(\"llama3\", \"The sky is...\", stream = FALSE, output = \"jsonlist\") }"},{"path":"https://hauselin.github.io/ollama-r/reference/image_encode_base64.html","id":null,"dir":"Reference","previous_headings":"","what":"Read image file and encode it to base64. — image_encode_base64","title":"Read image file and encode it to base64. — image_encode_base64","text":"Read image file encode base64.","code":""},{"path":"https://hauselin.github.io/ollama-r/reference/image_encode_base64.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Read image file and encode it to base64. — image_encode_base64","text":"","code":"image_encode_base64(image_path)"},{"path":"https://hauselin.github.io/ollama-r/reference/image_encode_base64.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Read image file and encode it to base64. — image_encode_base64","text":"image_path path image file.","code":""},{"path":"https://hauselin.github.io/ollama-r/reference/image_encode_base64.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Read image file and encode it to base64. — image_encode_base64","text":"base64 encoded string.","code":""},{"path":"https://hauselin.github.io/ollama-r/reference/image_encode_base64.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Read image file and encode it to base64. — image_encode_base64","text":"","code":"image_path <- file.path(system.file(\"extdata\", package = \"ollamar\"), \"image1.png\") substr(image_encode_base64(image_path), 1, 5) # truncate output #> [1] \"iVBOR\""},{"path":"https://hauselin.github.io/ollama-r/reference/insert_message.html","id":null,"dir":"Reference","previous_headings":"","what":"Insert message into a list at a specified position — insert_message","title":"Insert message into a list at a specified position — insert_message","text":"Inserts message specified position list messages. role content converted list inserted input list given position.","code":""},{"path":"https://hauselin.github.io/ollama-r/reference/insert_message.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Insert message into a list at a specified position — insert_message","text":"","code":"insert_message(content, role = \"user\", x = NULL, position = -1)"},{"path":"https://hauselin.github.io/ollama-r/reference/insert_message.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Insert message into a list at a specified position — insert_message","text":"content content message. role role message. Can \"user\", \"system\", \"assistant\". Default \"user\". x list messages. Default NULL. position position insert new message. Default -1 (end list).","code":""},{"path":"https://hauselin.github.io/ollama-r/reference/insert_message.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Insert message into a list at a specified position — insert_message","text":"list messages new message inserted specified position.","code":""},{"path":"https://hauselin.github.io/ollama-r/reference/insert_message.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Insert message into a list at a specified position — insert_message","text":"","code":"messages <- list(     list(role = \"system\", content = \"Be friendly\"),     list(role = \"user\", content = \"How are you?\") ) insert_message(\"INSERT MESSAGE AT THE END\", \"user\", messages) #> [[1]] #> [[1]]$role #> [1] \"system\" #>  #> [[1]]$content #> [1] \"Be friendly\" #>  #>  #> [[2]] #> [[2]]$role #> [1] \"user\" #>  #> [[2]]$content #> [1] \"How are you?\" #>  #>  #> [[3]] #> [[3]]$role #> [1] \"user\" #>  #> [[3]]$content #> [1] \"INSERT MESSAGE AT THE END\" #>  #>  insert_message(\"INSERT MESSAGE AT THE BEGINNING\", \"user\", messages, 2) #> [[1]] #> [[1]]$role #> [1] \"system\" #>  #> [[1]]$content #> [1] \"Be friendly\" #>  #>  #> [[2]] #> [[2]]$role #> [1] \"user\" #>  #> [[2]]$content #> [1] \"INSERT MESSAGE AT THE BEGINNING\" #>  #>  #> [[3]] #> [[3]]$role #> [1] \"user\" #>  #> [[3]]$content #> [1] \"How are you?\" #>  #>"},{"path":"https://hauselin.github.io/ollama-r/reference/list_models.html","id":null,"dir":"Reference","previous_headings":"","what":"Get available local models — list_models","title":"Get available local models — list_models","text":"Get available local models","code":""},{"path":"https://hauselin.github.io/ollama-r/reference/list_models.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get available local models — list_models","text":"","code":"list_models(   output = c(\"df\", \"resp\", \"jsonlist\", \"raw\", \"text\"),   endpoint = \"/api/tags\",   host = NULL )"},{"path":"https://hauselin.github.io/ollama-r/reference/list_models.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get available local models — list_models","text":"output output format. Default \"df\". options \"resp\", \"jsonlist\", \"raw\", \"text\". endpoint endpoint get models. Default \"/api/tags\". host base URL use. Default NULL, uses Ollama's default base URL.","code":""},{"path":"https://hauselin.github.io/ollama-r/reference/list_models.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get available local models — list_models","text":"response format specified output parameter.","code":""},{"path":"https://hauselin.github.io/ollama-r/reference/list_models.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Get available local models — list_models","text":"API documentation","code":""},{"path":"https://hauselin.github.io/ollama-r/reference/list_models.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Get available local models — list_models","text":"","code":"if (FALSE) { # test_connection()$status_code == 200 list_models() # returns dataframe list_models(\"df\") # returns dataframe list_models(\"resp\") # httr2 response object list_models(\"jsonlist\") list_models(\"raw\") }"},{"path":"https://hauselin.github.io/ollama-r/reference/model_avail.html","id":null,"dir":"Reference","previous_headings":"","what":"Check if model is available locally. — model_avail","title":"Check if model is available locally. — model_avail","text":"Check model available locally.","code":""},{"path":"https://hauselin.github.io/ollama-r/reference/model_avail.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Check if model is available locally. — model_avail","text":"","code":"model_avail(model)"},{"path":"https://hauselin.github.io/ollama-r/reference/model_avail.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Check if model is available locally. — model_avail","text":"model character string model name \"llama3\".","code":""},{"path":"https://hauselin.github.io/ollama-r/reference/model_avail.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Check if model is available locally. — model_avail","text":"logical value indicating model exists.","code":""},{"path":"https://hauselin.github.io/ollama-r/reference/model_avail.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Check if model is available locally. — model_avail","text":"","code":"if (FALSE) { # test_connection()$status_code == 200 model_avail(\"codegemma:7b\") model_avail(\"abc\") model_avail(\"llama3\") }"},{"path":"https://hauselin.github.io/ollama-r/reference/model_options.html","id":null,"dir":"Reference","previous_headings":"","what":"Model options — model_options","title":"Model options — model_options","text":"Model options","code":""},{"path":"https://hauselin.github.io/ollama-r/reference/model_options.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Model options — model_options","text":"","code":"model_options"},{"path":"https://hauselin.github.io/ollama-r/reference/model_options.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Model options — model_options","text":"object class list length 13.","code":""},{"path":"https://hauselin.github.io/ollama-r/reference/ohelp.html","id":null,"dir":"Reference","previous_headings":"","what":"Chat with a model in real-time in R console. — ohelp","title":"Chat with a model in real-time in R console. — ohelp","text":"Chat model real-time R console.","code":""},{"path":"https://hauselin.github.io/ollama-r/reference/ohelp.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Chat with a model in real-time in R console. — ohelp","text":"","code":"ohelp(model = \"codegemma:7b\", ...)"},{"path":"https://hauselin.github.io/ollama-r/reference/ohelp.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Chat with a model in real-time in R console. — ohelp","text":"model character string model name \"llama3\". Defaults \"codegemma:7b\" decent coding model 2024-07-27. ... Additional options. options currently available time.","code":""},{"path":"https://hauselin.github.io/ollama-r/reference/ohelp.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Chat with a model in real-time in R console. — ohelp","text":"return anything. prints conversation console.","code":""},{"path":"https://hauselin.github.io/ollama-r/reference/ohelp.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Chat with a model in real-time in R console. — ohelp","text":"","code":"if (FALSE) { # test_connection()$status_code == 200 ohelp(first_prompt = \"quit\") # regular usage: ohelp() }"},{"path":"https://hauselin.github.io/ollama-r/reference/package_config.html","id":null,"dir":"Reference","previous_headings":"","what":"Package configuration — package_config","title":"Package configuration — package_config","text":"Package configuration","code":""},{"path":"https://hauselin.github.io/ollama-r/reference/package_config.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Package configuration — package_config","text":"","code":"package_config"},{"path":"https://hauselin.github.io/ollama-r/reference/package_config.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Package configuration — package_config","text":"object class list length 3.","code":""},{"path":"https://hauselin.github.io/ollama-r/reference/prepend_message.html","id":null,"dir":"Reference","previous_headings":"","what":"Prepend message to a list — prepend_message","title":"Prepend message to a list — prepend_message","text":"Prepends message (add beginning list) list messages. role content converted list prepended input list.","code":""},{"path":"https://hauselin.github.io/ollama-r/reference/prepend_message.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Prepend message to a list — prepend_message","text":"","code":"prepend_message(content, role = \"user\", x = NULL)"},{"path":"https://hauselin.github.io/ollama-r/reference/prepend_message.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Prepend message to a list — prepend_message","text":"content content message. role role message. Can \"user\", \"system\", \"assistant\". x list messages. Default NULL.","code":""},{"path":"https://hauselin.github.io/ollama-r/reference/prepend_message.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Prepend message to a list — prepend_message","text":"list messages new message prepended.","code":""},{"path":"https://hauselin.github.io/ollama-r/reference/prepend_message.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Prepend message to a list — prepend_message","text":"","code":"prepend_message(\"user\", \"Hello\") #> [[1]] #> [[1]]$role #> [1] \"Hello\" #>  #> [[1]]$content #> [1] \"user\" #>  #>  prepend_message(\"system\", \"Always respond nicely\") #> [[1]] #> [[1]]$role #> [1] \"Always respond nicely\" #>  #> [[1]]$content #> [1] \"system\" #>  #>"},{"path":"https://hauselin.github.io/ollama-r/reference/pull.html","id":null,"dir":"Reference","previous_headings":"","what":"Pull/download a model — pull","title":"Pull/download a model — pull","text":"See https://ollama.com/library list available models. Use list_models() function get list models already downloaded/installed machine.","code":""},{"path":"https://hauselin.github.io/ollama-r/reference/pull.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Pull/download a model — pull","text":"","code":"pull(   name,   stream = TRUE,   insecure = FALSE,   endpoint = \"/api/pull\",   host = NULL )"},{"path":"https://hauselin.github.io/ollama-r/reference/pull.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Pull/download a model — pull","text":"name character string model name download/pull, \"llama3\". stream Enable response streaming. Default TRUE. insecure Allow insecure connections use pulling library development. Default FALSE. endpoint endpoint pull model. Default \"/api/pull\". host base URL use. Default NULL, uses Ollama's default base URL.","code":""},{"path":"https://hauselin.github.io/ollama-r/reference/pull.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Pull/download a model — pull","text":"httr2 response object.","code":""},{"path":"https://hauselin.github.io/ollama-r/reference/pull.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Pull/download a model — pull","text":"API documentation","code":""},{"path":"https://hauselin.github.io/ollama-r/reference/pull.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Pull/download a model — pull","text":"","code":"if (FALSE) { # test_connection()$status_code == 200 pull(\"llama3\") pull(\"all-minilm\", stream = FALSE) }"},{"path":"https://hauselin.github.io/ollama-r/reference/resp_process.html","id":null,"dir":"Reference","previous_headings":"","what":"Process httr2 response object. — resp_process","title":"Process httr2 response object. — resp_process","text":"Process httr2 response object.","code":""},{"path":"https://hauselin.github.io/ollama-r/reference/resp_process.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Process httr2 response object. — resp_process","text":"","code":"resp_process(resp, output = c(\"df\", \"jsonlist\", \"raw\", \"resp\", \"text\"))"},{"path":"https://hauselin.github.io/ollama-r/reference/resp_process.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Process httr2 response object. — resp_process","text":"resp httr2 response object. output output format. Default \"df\". options \"jsonlist\", \"raw\", \"resp\" (httr2 response object), \"text\"","code":""},{"path":"https://hauselin.github.io/ollama-r/reference/resp_process.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Process httr2 response object. — resp_process","text":"data frame, json list, raw httr2 response object.","code":""},{"path":"https://hauselin.github.io/ollama-r/reference/resp_process.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Process httr2 response object. — resp_process","text":"","code":"if (FALSE) { # test_connection()$status_code == 200 resp <- list_models(\"resp\") resp_process(resp, \"df\") # parse response to dataframe/tibble resp_process(resp, \"jsonlist\") # parse response to list resp_process(resp, \"raw\") # parse response to raw string resp_process(resp, \"resp\") # return input response object resp_process(resp, \"text\") # return text/character vector }"},{"path":"https://hauselin.github.io/ollama-r/reference/resp_process_stream.html","id":null,"dir":"Reference","previous_headings":"","what":"Process httr2 response object for streaming. — resp_process_stream","title":"Process httr2 response object for streaming. — resp_process_stream","text":"Process httr2 response object streaming.","code":""},{"path":"https://hauselin.github.io/ollama-r/reference/resp_process_stream.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Process httr2 response object for streaming. — resp_process_stream","text":"","code":"resp_process_stream(resp, output)"},{"path":"https://hauselin.github.io/ollama-r/reference/search_options.html","id":null,"dir":"Reference","previous_headings":"","what":"Search for options based on a query. — search_options","title":"Search for options based on a query. — search_options","text":"Search options based query.","code":""},{"path":"https://hauselin.github.io/ollama-r/reference/search_options.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Search for options based on a query. — search_options","text":"","code":"search_options(query)"},{"path":"https://hauselin.github.io/ollama-r/reference/search_options.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Search for options based on a query. — search_options","text":"query query (character) search options.","code":""},{"path":"https://hauselin.github.io/ollama-r/reference/search_options.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Search for options based on a query. — search_options","text":"Returns list matching options.","code":""},{"path":"https://hauselin.github.io/ollama-r/reference/search_options.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Search for options based on a query. — search_options","text":"","code":"search_options(\"learning rate\") #> Matching options: mirostat_eta  #> $mirostat_eta #> $mirostat_eta$description #> [1] \"Influences how quickly the algorithm responds to feedback from the generated text. A lower learning rate will result in slower adjustments, while a higher learning rate will make the algorithm more responsive.\" #>  #> $mirostat_eta$default_value #> [1] 0.1 #>  #>  search_options(\"tokens\") #> Matching options: tfs_z, num_predict  #> $tfs_z #> $tfs_z$description #> [1] \"Tail free sampling is used to reduce the impact of less probable tokens from the output. A higher value (e.g., 2.0) will reduce the impact more, while a value of 1.0 disables this setting.\" #>  #> $tfs_z$default_value #> [1] 1 #>  #>  #> $num_predict #> $num_predict$description #> [1] \"Maximum number of tokens to predict when generating text. (Default: 128, -1 = infinite generation, -2 = fill context)\" #>  #> $num_predict$default_value #> [1] 128 #>  #>  search_options(\"invalid query\") #> No matching options found #>  #> list()"},{"path":"https://hauselin.github.io/ollama-r/reference/show.html","id":null,"dir":"Reference","previous_headings":"","what":"Show model information — show","title":"Show model information — show","text":"Show model information","code":""},{"path":"https://hauselin.github.io/ollama-r/reference/show.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Show model information — show","text":"","code":"show(   name,   verbose = FALSE,   output = c(\"jsonlist\", \"resp\", \"raw\"),   endpoint = \"/api/show\",   host = NULL )"},{"path":"https://hauselin.github.io/ollama-r/reference/show.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Show model information — show","text":"name Name model show verbose Returns full data verbose response fields. Default FALSE. output output format. Default \"jsonlist\". options \"resp\", \"raw\". endpoint endpoint show model. Default \"/api/show\". host base URL use. Default NULL, uses Ollama's default base URL.","code":""},{"path":"https://hauselin.github.io/ollama-r/reference/show.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Show model information — show","text":"response format specified output parameter.","code":""},{"path":"https://hauselin.github.io/ollama-r/reference/show.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Show model information — show","text":"API documentation","code":""},{"path":"https://hauselin.github.io/ollama-r/reference/show.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Show model information — show","text":"","code":"if (FALSE) { # test_connection()$status_code == 200 # show(\"llama3\") # returns jsonlist show(\"llama3\", output = \"resp\") # returns response object }"},{"path":"https://hauselin.github.io/ollama-r/reference/stream_handler.html","id":null,"dir":"Reference","previous_headings":"","what":"Stream handler helper function — stream_handler","title":"Stream handler helper function — stream_handler","text":"Function handle streaming.","code":""},{"path":"https://hauselin.github.io/ollama-r/reference/stream_handler.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Stream handler helper function — stream_handler","text":"","code":"stream_handler(x, env, endpoint)"},{"path":"https://hauselin.github.io/ollama-r/reference/test_connection.html","id":null,"dir":"Reference","previous_headings":"","what":"Test connection to Ollama server — test_connection","title":"Test connection to Ollama server — test_connection","text":"Test connection Ollama server","code":""},{"path":"https://hauselin.github.io/ollama-r/reference/test_connection.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Test connection to Ollama server — test_connection","text":"","code":"test_connection(url = \"http://localhost:11434\")"},{"path":"https://hauselin.github.io/ollama-r/reference/test_connection.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Test connection to Ollama server — test_connection","text":"url URL Ollama server. Default http://localhost:11434","code":""},{"path":"https://hauselin.github.io/ollama-r/reference/test_connection.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Test connection to Ollama server — test_connection","text":"httr2 response object.","code":""},{"path":"https://hauselin.github.io/ollama-r/reference/test_connection.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Test connection to Ollama server — test_connection","text":"","code":"test_connection() #> Ollama local server not running or wrong server. #> Download and launch Ollama app to run the server. Visit https://ollama.com or https://github.com/ollama/ollama #> <httr2_request> #> GET http://localhost:11434 #> Body: empty test_connection(\"http://localhost:11434\") # default url #> Ollama local server not running or wrong server. #> Download and launch Ollama app to run the server. Visit https://ollama.com or https://github.com/ollama/ollama #> <httr2_request> #> GET http://localhost:11434 #> Body: empty test_connection(\"http://127.0.0.1:11434\") #> Ollama local server not running or wrong server. #> Download and launch Ollama app to run the server. Visit https://ollama.com or https://github.com/ollama/ollama #> <httr2_request> #> GET http://127.0.0.1:11434 #> Body: empty"},{"path":"https://hauselin.github.io/ollama-r/reference/validate_options.html","id":null,"dir":"Reference","previous_headings":"","what":"Validate additional options or parameters provided to the API call — validate_options","title":"Validate additional options or parameters provided to the API call — validate_options","text":"Validate additional options parameters provided API call","code":""},{"path":"https://hauselin.github.io/ollama-r/reference/validate_options.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Validate additional options or parameters provided to the API call — validate_options","text":"","code":"validate_options(...)"},{"path":"https://hauselin.github.io/ollama-r/reference/validate_options.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Validate additional options or parameters provided to the API call — validate_options","text":"... Additional options parameters provided API call","code":""},{"path":"https://hauselin.github.io/ollama-r/reference/validate_options.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Validate additional options or parameters provided to the API call — validate_options","text":"TRUE additional options valid, FALSE otherwise","code":""},{"path":"https://hauselin.github.io/ollama-r/reference/validate_options.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Validate additional options or parameters provided to the API call — validate_options","text":"","code":"validate_options(mirostat = 1, mirostat_eta = 0.2, num_ctx = 1024) #> [1] TRUE validate_options(mirostat = 1, mirostat_eta = 0.2, invalid_opt = 1024) #> Valid options: mirostat, mirostat_eta #> Invalid options: invalid_opt #> See available options with check_options() or model_options. #> See also https://github.com/ollama/ollama/blob/main/docs/modelfile.md#parameter #> [1] FALSE"},{"path":"https://hauselin.github.io/ollama-r/news/index.html","id":"ollamar-111","dir":"Changelog","previous_headings":"","what":"ollamar 1.1.1","title":"ollamar 1.1.1","text":"CRAN release: 2024-05-02","code":""},{"path":"https://hauselin.github.io/ollama-r/news/index.html","id":"bug-fixes-1-1-1","dir":"Changelog","previous_headings":"","what":"Bug fixes","title":"ollamar 1.1.1","text":"Fixed invalid URLs. Updated title description.","code":""},{"path":"https://hauselin.github.io/ollama-r/news/index.html","id":"ollamar-100","dir":"Changelog","previous_headings":"","what":"ollamar 1.0.0","title":"ollamar 1.0.0","text":"Initial CRAN submission.","code":""},{"path":"https://hauselin.github.io/ollama-r/news/index.html","id":"new-features-1-0-0","dir":"Changelog","previous_headings":"","what":"New features","title":"ollamar 1.0.0","text":"Integrated R Ollama run language models locally machine. Included test_connection() function test connection Ollama server. Included list_models() function list available models. Included pull() function pull model Ollama server. Included delete() function delete model Ollama server. Included chat() function chat model. Included generate() function generate text model. Included embeddings() function get embeddings model. Included resp_process() function process httr2_response objects.","code":""}]
